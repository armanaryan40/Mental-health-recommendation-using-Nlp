{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kuzbcg8XRjFc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = pd.read_csv('data.csv').iloc[:5000]"
      ],
      "metadata": {
        "id": "wMqmYZRJTO9W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFewOzpmTYIi",
        "outputId": "d03e554e-9a04-4efe-a893-ce87dae20982"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'text', 'class'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data['class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "AdS9z3uRTagY",
        "outputId": "8bc4d37e-cd63-40cb-de18-e7d8050f54df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class\n",
              "non-suicide    2531\n",
              "suicide        2469\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>non-suicide</th>\n",
              "      <td>2531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>suicide</th>\n",
              "      <td>2469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv('data_mental_health.csv', index=False)"
      ],
      "metadata": {
        "id": "WNE17d9xTihO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJmRjk7RTsKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach- 1: TFIDF Embeddings"
      ],
      "metadata": {
        "id": "mBTxGpCRbND2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Understanding TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
        "\n",
        "TF-IDF stands for **Term Frequency - Inverse Document Frequency**, a statistical measure used to evaluate the importance of a word (term) in a document relative to a collection of documents (corpus). Unlike word embeddings, which are learned dense vector representations capturing semantic meaning, TF-IDF is a non-semantic, traditional approach for representing text data. Here’s an in-depth explanation of the concept and how it relates to what we discussed earlier.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is TF-IDF?**\n",
        "\n",
        "TF-IDF is a technique that combines two metrics:\n",
        "1. **Term Frequency (TF)**: Measures how frequently a term appears in a document.\n",
        "2. **Inverse Document Frequency (IDF)**: Measures how unique or rare the term is across all documents in the corpus.\n",
        "\n",
        "### **a. Term Frequency (TF)**\n",
        "- **Definition**: It refers to how many times a term appears in a specific document. The more often the term appears, the higher its TF value.\n",
        "- **Intuitive Explanation**: For example, if we’re analyzing the sentence \"The cat sat on the mat,\" the word \"the\" appears twice, so its term frequency is high.\n",
        "\n",
        "### **b. Inverse Document Frequency (IDF)**\n",
        "- **Definition**: It measures how common or rare a term is across all documents in the corpus. Terms that appear frequently across multiple documents have lower IDF values, as they are considered less informative.\n",
        "- **Intuitive Explanation**: Words like \"the,\" \"is,\" and \"and\" are common across many documents and thus have lower IDF values. On the other hand, specialized terms like \"neurotransmitter\" or \"quantum\" might appear in fewer documents, giving them higher IDF values.\n",
        "\n",
        "### **c. Combined TF-IDF**\n",
        "- **Definition**: The TF-IDF score for a term in a document is obtained by multiplying its TF and IDF scores.\n",
        "- **Purpose**: TF-IDF scores highlight terms that are important within a document but less common across the entire corpus. This helps in identifying words that uniquely describe the content of a document.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. TF-IDF: Semantic vs. Non-Semantic Embeddings**\n",
        "\n",
        "### **TF-IDF as a Non-Semantic Embedding**\n",
        "- **Non-Semantic Nature**: Unlike semantic embeddings (e.g., BERT or Word2Vec) that capture the contextual meaning of words, TF-IDF does not understand the semantics or relationships between words. Instead, it simply assigns weights to words based on their frequency within a document and across the corpus.\n",
        "- **Focus on Frequency**: TF-IDF is a statistical representation that prioritizes words that are specific to a particular document. It does not capture meaning, but it is effective in identifying significant words that can distinguish one document from another.\n",
        "\n",
        "### **Example Comparison:**\n",
        "1. **TF-IDF Approach**:\n",
        "   - **Word-Level Representation**: \"The cat sat on the mat\" and \"The dog slept on the mat\" would have different TF-IDF scores for \"cat\" and \"dog,\" even though the context may be similar.\n",
        "   - **No Context Awareness**: TF-IDF assigns weights purely based on term frequency, without understanding that \"cat\" and \"dog\" could be similar in context.\n",
        "   \n",
        "2. **Semantic Embeddings (e.g., Word2Vec, BERT)**:\n",
        "   - **Contextual Representation**: Embeddings would place \"cat\" and \"dog\" close together in vector space because they are semantically similar.\n",
        "   - **Context Awareness**: Unlike TF-IDF, semantic embeddings understand and encode the meaning of words based on their usage in a sentence.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "okFsFNvVa8g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_data.copy()"
      ],
      "metadata": {
        "id": "euy4D4G7bCQ7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the dataset\n",
        "df.info()\n",
        "\n",
        "# Check for missing values\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "mbj71CoCbf3y",
        "outputId": "641e97fe-ee72-485c-c220-1234734ff094"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  5000 non-null   int64 \n",
            " 1   text        5000 non-null   object\n",
            " 2   class       5000 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 117.3+ KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0    0\n",
              "text          0\n",
              "class         0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n"
      ],
      "metadata": {
        "id": "yZ6r3xN5bjfi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tfidf_vectorizer.fit_transform(df['text'].fillna('')).toarray()\n"
      ],
      "metadata": {
        "id": "VFUIwZGJbp5b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'label' is the target column\n",
        "y = df['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "nqV1FgjxbtG4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train the classifier\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "xpgybai6bzEq",
        "outputId": "ee8af321-82ce-48a2-c442-33a4d25a529f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGPzse-Bb5bt",
        "outputId": "df580f51-ed51-4ce5-88de-a92317fc83e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.898\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non-suicide       0.88      0.93      0.90       520\n",
            "     suicide       0.92      0.86      0.89       480\n",
            "\n",
            "    accuracy                           0.90      1000\n",
            "   macro avg       0.90      0.90      0.90      1000\n",
            "weighted avg       0.90      0.90      0.90      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new example text\n",
        "new_text = [\"I feel hopeless and don't see a way out.\"]\n",
        "\n",
        "# Preprocess and transform the new text using the trained TF-IDF vectorizer\n",
        "new_text_tfidf = tfidf_vectorizer.transform(new_text).toarray()\n",
        "\n",
        "# Use the trained classifier to predict the class\n",
        "predicted_class = classifier.predict(new_text_tfidf)\n",
        "\n",
        "# Display the prediction\n",
        "print(\"Predicted Class:\", predicted_class[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFCsvBkbb8o7",
        "outputId": "7e6a7b83-0582-4f3b-92c0-30e4b2d8d507"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: suicide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new example text\n",
        "new_text = [\"what a great day it is\"]\n",
        "\n",
        "# Preprocess and transform the new text using the trained TF-IDF vectorizer\n",
        "new_text_tfidf = tfidf_vectorizer.transform(new_text).toarray()\n",
        "\n",
        "# Use the trained classifier to predict the class\n",
        "predicted_class = classifier.predict(new_text_tfidf)\n",
        "\n",
        "# Display the prediction\n",
        "print(\"Predicted Class:\", predicted_class[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgBdTr_lcOfk",
        "outputId": "dfc14660-9845-47c6-e685-591247ccafcf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: non-suicide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = [\"Lately, I’ve been struggling a lot. There are days when I feel completely overwhelmed, like everything is crashing down around me, and I just want to escape. But then there are moments where I think maybe things could get better, that I might find a way through this. I’ve been trying to reach out to friends, and they’ve been supportive, but it’s hard to explain what I’m going through. Some days are okay, but other days, the darkness just feels too heavy to bear. I wish I could see a light at the end of the tunnel, but it’s not always there. I just don’t know what to do anymore.\"]"
      ],
      "metadata": {
        "id": "-DHXpsVwcTHK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and transform the new text using the trained TF-IDF vectorizer\n",
        "new_text_tfidf = tfidf_vectorizer.transform(new_text).toarray()\n",
        "\n",
        "# Use the trained classifier to predict the class\n",
        "predicted_class = classifier.predict(new_text_tfidf)\n",
        "\n",
        "# Display the prediction\n",
        "print(\"Predicted Class:\", predicted_class[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBiOYYvkchWi",
        "outputId": "a2f9ec3f-8313-468a-b036-6836e219bc07"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: suicide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = [\"I’ve been dealing with depression for a few years now, and it hasn’t been easy. There were times when I felt like giving up, but I found strength in seeking help. Therapy and talking to friends really made a difference. Now, I’m in a much better place, and I want to use my experience to support others who might be going through something similar. Mental health is so important, and I believe we need to talk about it openly. If sharing my story can encourage even one person to seek help, then it’s worth it.\"]"
      ],
      "metadata": {
        "id": "FOhimp0jcjlg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4M-6Uskc4hZ",
        "outputId": "00b009d2-96a7-48ee-a7b5-d1167e5295cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I’ve been dealing with depression for a few years now, and it hasn’t been easy. There were times when I felt like giving up, but I found strength in seeking help. Therapy and talking to friends really made a difference. Now, I’m in a much better place, and I want to use my experience to support others who might be going through something similar. Mental health is so important, and I believe we need to talk about it openly. If sharing my story can encourage even one person to seek help, then it’s worth it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and transform the new text using the trained TF-IDF vectorizer\n",
        "new_text_tfidf = tfidf_vectorizer.transform(new_text).toarray()\n",
        "\n",
        "# Use the trained classifier to predict the class\n",
        "predicted_class = classifier.predict(new_text_tfidf)\n",
        "\n",
        "# Display the prediction\n",
        "print(\"Predicted Class:\", predicted_class[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqXMh61cdAF1",
        "outputId": "e1fbdb83-adda-4a59-ef22-8569b2536df7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: suicide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BnW86YmdChm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Interpretation***:\n",
        "This result highlights a limitation of the TF-IDF-based model:\n",
        "\n",
        "- Keyword Sensitivity: The model might have picked up on words like \"depression,\" \"giving up,\" and \"mental health,\" which are often associated with distressing contexts. However, in this case, the text overall conveys a positive, supportive message.\n",
        "- Lack of Context Understanding: Since TF-IDF doesn't capture semantics, it can struggle with nuanced expressions where words commonly associated with negative situations are used in a positive or neutral context.\n",
        "\n",
        "***Conclusion***:\n",
        "The TF-IDF-based model performs well, achieving almost 90% accuracy. However, it is important to note that TF-IDF is a non-semantic method that doesn’t capture the meaning or context of words, unlike semantic embeddings like BERT. For further improvements, a semantic embedding approach could be considered to capture nuances and relationships between words more effectively.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4SX8DeupdNOI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJQUOh0sdNyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach- 2: Word Embeddings (Word2Vec- Dense Embeddings)"
      ],
      "metadata": {
        "id": "PPevjXrfeVOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Understanding Word2Vec**\n",
        "\n",
        "Word2Vec is a popular technique for generating word embeddings, which are dense vector representations of words. Unlike traditional methods like TF-IDF, which treat words as isolated units, Word2Vec captures the semantic meaning of words by considering their contexts. This makes it a **semantic embedding** approach, as it learns to understand relationships and similarities between words based on how they are used in sentences.\n",
        "\n",
        "Let’s explore Word2Vec in detail.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is Word2Vec?**\n",
        "\n",
        "Word2Vec is a neural network-based method that learns to represent words as continuous vectors in a multi-dimensional space. The key idea is that **words with similar meanings will have similar vector representations**. Word2Vec models are trained on large corpora, and through the training process, they learn to place semantically similar words close to each other in the vector space.\n",
        "\n",
        "### **Key Characteristics of Word2Vec:**\n",
        "- **Semantic Embeddings**: Word2Vec captures the meaning of words by analyzing the contexts in which they appear.\n",
        "- **Dense Vectors**: Each word is represented by a dense vector (e.g., 100-300 dimensions), unlike sparse representations in TF-IDF.\n",
        "- **Contextual Similarity**: Words that appear in similar contexts will have similar embeddings, allowing the model to understand word relationships.\n",
        "\n",
        "### **Example:**\n",
        "If we train a Word2Vec model on a large text corpus, it will learn to map:\n",
        "- **\"king\" - \"man\" + \"woman\" ≈ \"queen\"**\n",
        "- **\"apple\" and \"orange\"** close to each other in vector space, as they are both fruits.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How Word2Vec Works:**\n",
        "\n",
        "Word2Vec has two main training approaches:\n",
        "1. **Continuous Bag of Words (CBOW)**\n",
        "2. **Skip-Gram**\n",
        "\n",
        "### **a. Continuous Bag of Words (CBOW)**\n",
        "- **Objective**: Predict the target word given its surrounding context words.\n",
        "- **Mechanism**: The model learns to predict a word based on the words that come before and after it. For instance, in the sentence \"The cat sat on the mat,\" CBOW tries to predict \"sat\" from [\"the\", \"cat\", \"on\", \"the\"].\n",
        "- **Efficient for Frequent Words**: CBOW is faster and more suitable for frequent words because it learns from multiple contexts.\n",
        "\n",
        "### **b. Skip-Gram**\n",
        "- **Objective**: Predict the surrounding context words given a target word.\n",
        "- **Mechanism**: The model learns to predict words that are likely to appear around a given word. Using the same example, Skip-Gram would try to predict [\"the\", \"cat\", \"on\", \"the\"] given \"sat.\"\n",
        "- **Effective for Rare Words**: Skip-Gram is particularly effective for learning representations of rare words because it treats each word independently.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Semantic vs. Non-Semantic Embeddings: Comparison with TF-IDF**\n",
        "\n",
        "Word2Vec and TF-IDF represent two fundamentally different approaches to understanding text:\n",
        "\n",
        "### **Word2Vec (Semantic Embeddings)**\n",
        "1. **Contextual Awareness**: Word2Vec captures the meanings of words based on their usage in sentences. Words appearing in similar contexts will have similar embeddings, even if they are not identical.\n",
        "2. **Dense and Low-Dimensional**: The vectors are continuous and lower-dimensional (e.g., 100-300 dimensions), allowing for meaningful comparisons like word analogies.\n",
        "3. **Captures Relationships**: Can understand relationships like \"king\" is to \"queen\" as \"man\" is to \"woman,\" which are impossible to capture with TF-IDF.\n",
        "\n",
        "### **TF-IDF (Non-Semantic Embeddings)**\n",
        "1. **No Contextual Understanding**: TF-IDF is based purely on term frequency. It doesn’t understand that \"car\" and \"automobile\" are similar; it treats them as entirely different words.\n",
        "2. **Sparse and High-Dimensional**: TF-IDF vectors are usually sparse (most values are zeros) and high-dimensional (one dimension per word in the vocabulary).\n",
        "3. **Limited to Frequency**: TF-IDF doesn’t capture relationships between words. It only highlights which words are important in a document relative to the corpus.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Practical Applications of Word2Vec:**\n",
        "- **Semantic Search**: Word2Vec can be used to improve search results by understanding synonyms and context.\n",
        "- **Recommendation Systems**: By understanding the relationships between products or items, Word2Vec can suggest similar items based on user preferences.\n",
        "- **Text Classification**: Pre-trained Word2Vec embeddings can be used to convert text data into vectors for various classification tasks.\n",
        "\n",
        "### **Example of Word Similarities:**\n",
        "In a Word2Vec model trained on a large corpus, similar words are grouped together:\n",
        "- **\"happy\"** and **\"joyful\"** would be close to each other.\n",
        "- **\"sad\"** and **\"unhappy\"** would be grouped separately from the positive words.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WQugLYOPeLkx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PglbplK7eNtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Contextual vs. Non-Contextual Embeddings**\n",
        "\n",
        "**Word2Vec (Non-Contextual Embeddings):**\n",
        "- **Fixed Word Representations**: In Word2Vec, each word has a single, fixed vector representation, regardless of the context in which it appears. For example, the word \"bank\" will have the same embedding whether it appears in \"river bank\" or \"financial bank.\"\n",
        "- **Local Context Understanding**: Word2Vec captures meaning by looking at the words around the target word, but it doesn’t adjust the representation based on different sentence contexts. It learns semantic similarities by examining co-occurrences, meaning words that often appear together will have similar embeddings.\n",
        "\n",
        "**BERT/Transformers (Contextual Embeddings):**\n",
        "- **Dynamic Word Representations**: BERT and other transformer-based models generate embeddings that change based on the context. For example, the word \"bank\" will have different vectors depending on whether it’s used in the context of a river or a financial institution.\n",
        "- **Deep Contextual Understanding**: Transformers process the entire sentence (or even multiple sentences) simultaneously. This means that the embeddings for each word are influenced by all the words around it, leading to a more accurate understanding of meaning.\n",
        "\n",
        "**Key Difference**: Word2Vec provides a single, static embedding per word, while BERT gives different embeddings for the same word depending on its usage.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Training Approach and Model Architecture**\n",
        "\n",
        "**Word2Vec:**\n",
        "- **Shallow Neural Network**: Word2Vec uses a shallow, two-layer neural network. The training is relatively fast and focuses on learning the co-occurrence statistics of words within a local window (e.g., 5-10 words).\n",
        "- **Training Objectives**: The two main approaches (CBOW and Skip-Gram) learn embeddings by predicting the target word from its context or vice versa.\n",
        "- **Efficiency**: Word2Vec is efficient to train and can still capture some level of semantic similarity by learning which words often appear together.\n",
        "\n",
        "**BERT/Transformers:**\n",
        "- **Deep Neural Network**: BERT uses a deep, transformer-based architecture with multiple layers of attention. This allows the model to capture more complex patterns and relationships between words.\n",
        "- **Self-Attention Mechanism**: Transformers use self-attention to learn how every word in a sentence relates to every other word, even when they are far apart. This enables BERT to understand dependencies that may not be immediately obvious.\n",
        "- **Bidirectional Training**: Unlike Word2Vec, which only considers a limited window around the target word, BERT processes the sentence in both directions (left-to-right and right-to-left). This bidirectional approach helps it gain a more holistic understanding of the context.\n",
        "\n",
        "**Key Difference**: Word2Vec uses a shallow network to learn local word co-occurrences, while BERT relies on a deep, transformer-based architecture that analyzes all words simultaneously, leading to richer and more nuanced understanding.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Handling Ambiguity and Polysemy**\n",
        "\n",
        "**Word2Vec:**\n",
        "- **Limited Understanding**: Since Word2Vec provides a single vector per word, it struggles with polysemous words (words with multiple meanings). For instance, \"bat\" (the animal) and \"bat\" (used in sports) will be represented by the same vector.\n",
        "- **Context Not Differentiated**: Because it cannot differentiate between contexts, Word2Vec cannot distinguish between meanings based on the sentence.\n",
        "\n",
        "**BERT/Transformers:**\n",
        "- **Contextual Sensitivity**: BERT excels at handling ambiguity. Since it generates dynamic embeddings based on context, it can differentiate between different meanings of the same word. For example, BERT will give different vectors to \"bat\" depending on whether it appears in \"a baseball bat\" or \"a nocturnal bat.\"\n",
        "- **Better at Disambiguation**: The attention mechanism helps BERT understand the specific context, allowing it to disambiguate words and phrases effectively.\n",
        "\n",
        "**Key Difference**: Word2Vec provides the same embedding for a word regardless of its usage, while BERT creates different embeddings based on the word’s context, leading to more accurate semantic understanding.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Understanding Phrases, Sentences, and Long-Term Dependencies**\n",
        "\n",
        "**Word2Vec:**\n",
        "- **Word-Level Understanding**: Word2Vec operates at the word level, without inherently understanding phrases or sentences. It doesn’t capture how words interact across longer contexts.\n",
        "- **Limited to Local Context**: Word2Vec learns based on co-occurrence within a limited window, making it less effective at understanding long-range dependencies or the overall meaning of a sentence.\n",
        "\n",
        "**BERT/Transformers:**\n",
        "- **Phrase and Sentence-Level Understanding**: BERT and transformers can generate embeddings for phrases, sentences, and even longer pieces of text. They understand how words contribute to the meaning of a whole phrase or sentence.\n",
        "- **Handles Long-Term Dependencies**: Through self-attention, BERT can capture dependencies across a sentence, understanding how words separated by many others are still related. This allows it to comprehend complex structures, idiomatic expressions, and nuanced meanings.\n",
        "\n",
        "**Key Difference**: Word2Vec focuses on word-level co-occurrence, while BERT can analyze entire phrases and sentences, understanding long-term dependencies and relationships.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Pre-Training vs. Fine-Tuning Capabilities**\n",
        "\n",
        "**Word2Vec:**\n",
        "- **Pre-Trained Embeddings**: Once Word2Vec generates embeddings, they are fixed and can be used directly for various tasks. You can download pre-trained Word2Vec models and use them as static word embeddings.\n",
        "- **No Fine-Tuning**: Word2Vec does not adapt its embeddings based on specific tasks. The vectors are static and not fine-tuned further.\n",
        "\n",
        "**BERT/Transformers:**\n",
        "- **Pre-Training and Fine-Tuning**: BERT is pre-trained on massive text corpora (like Wikipedia) to understand language patterns. After pre-training, BERT can be fine-tuned on specific tasks (like sentiment analysis, question-answering) to adapt its embeddings for that task.\n",
        "- **Task-Specific Adaptation**: This ability to fine-tune makes BERT incredibly versatile and powerful. For example, it can adapt its understanding for medical texts, legal documents, or general conversations depending on the task.\n",
        "\n",
        "**Key Difference**: Word2Vec provides static embeddings that do not change, while BERT can be fine-tuned to adapt to specific tasks, making it more versatile and powerful.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Summary Table: Word2Vec vs. BERT/Transformers**\n",
        "\n",
        "| **Feature**                 | **Word2Vec**                                                  | **BERT/Transformers**                                       |\n",
        "|-----------------------------|--------------------------------------------------------------|-------------------------------------------------------------|\n",
        "| **Context Sensitivity**     | Non-Contextual: Same embedding for a word regardless of context | Contextual: Embeddings change based on context             |\n",
        "| **Training Approach**       | Shallow neural network (CBOW, Skip-Gram)                     | Deep transformer architecture with self-attention           |\n",
        "| **Handling Ambiguity**      | Limited: Cannot differentiate meanings of polysemous words   | Effective: Different embeddings based on context            |\n",
        "| **Phrase/Sentence Understanding** | Word-level, limited understanding of phrases           | Sentence-level, capable of understanding complex structures |\n",
        "| **Fine-Tuning**             | Static embeddings, no task-specific adaptation               | Can be fine-tuned for specific tasks                        |\n",
        "| **Efficiency**              | Faster to train, less computationally intensive              | Slower to train, computationally intensive                  |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "- **Word2Vec** is a powerful tool for learning word relationships, and it served as a major advancement over earlier approaches like TF-IDF. However, it lacks the ability to understand context and differentiate meanings based on sentence structure.\n",
        "- **BERT and Transformers** revolutionized NLP by introducing context-aware embeddings. They can capture subtle nuances in language, understand phrases and sentences, and adapt to specific tasks through fine-tuning.\n",
        "\n",
        "Both methods have their use cases:\n",
        "- Use **Word2Vec** if you need fast, efficient, and general-purpose word embeddings.\n",
        "- Use **BERT/Transformers** if you require deep, context-aware understanding for complex language tasks.\n"
      ],
      "metadata": {
        "id": "FngOBo49eros"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a tabular comparison of **semantic embeddings** (like Word2Vec) and **contextual embeddings** (like BERT and other transformer-based models):\n",
        "\n",
        "| **Feature**                 | **Semantic Embeddings** (e.g., Word2Vec)                | **Contextual Embeddings** (e.g., BERT, Transformers)              |\n",
        "|-----------------------------|---------------------------------------------------------|-------------------------------------------------------------------|\n",
        "| **Representation**          | Static (one fixed vector per word)                     | Dynamic (vector changes based on the context)                     |\n",
        "| **Understanding**           | Basic semantic similarity based on co-occurrence       | Deep, nuanced understanding based on sentence-level context       |\n",
        "| **Context Sensitivity**     | Not context-sensitive; \"bank\" has the same vector in \"river bank\" and \"financial bank\" | Context-sensitive; \"bank\" will have different vectors in \"river bank\" and \"financial bank\" |\n",
        "| **Training Objective**      | Predicting word relationships through local context (CBOW/Skip-Gram) | Self-supervised learning with tasks like Masked Language Modeling (MLM) |\n",
        "| **Scope of Learning**       | Word-level; learns relationships between individual words | Sentence-level and beyond; captures interactions between words across a sentence or document |\n",
        "| **Handling Polysemy**       | Cannot distinguish different meanings of a word        | Can differentiate meanings based on context (e.g., polysemy)      |\n",
        "| **Model Architecture**      | Shallow neural network (CBOW/Skip-Gram)                | Deep transformer-based architecture with multiple attention layers |\n",
        "| **Efficiency**              | Faster to train and use; efficient for simple tasks    | Slower to train; computationally intensive but highly accurate    |\n",
        "| **Ability to Capture Relationships** | Captures basic word associations (e.g., analogies) | Captures complex dependencies, relationships, and structures     |\n",
        "| **Pre-Training vs. Fine-Tuning** | Static pre-trained vectors; no task-specific adaptation | Can be pre-trained on general data and fine-tuned for specific tasks |\n",
        "| **Use Cases**               | General-purpose word similarity, basic NLP tasks      | Complex NLP tasks like sentiment analysis, translation, Q&A      |\n",
        "| **Example of Use**          | \"king\" - \"man\" + \"woman\" ≈ \"queen\"                     | Correctly interpreting \"The bank raised interest rates\" vs. \"The boat is at the river bank\" |\n",
        "\n",
        "### **Summary**\n",
        "- **Semantic Embeddings** provide a basic level of understanding by learning which words are similar based on co-occurrences. They are **static** and do not change based on different uses of a word.\n",
        "- **Contextual Embeddings** go further by generating **dynamic** representations that adapt to the word’s role in a particular sentence, enabling a deeper understanding of meaning and context. This makes them suitable for more complex NLP tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "2D-9qMAVfRny"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LL6YL6KfS7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec coding"
      ],
      "metadata": {
        "id": "3y3thrsqgDPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = 'data_mental_health.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop unnecessary columns if any (e.g., index column)\n",
        "df_cleaned = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
        "\n",
        "# Step 2: Preprocess the text\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words using NLTK's stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Apply the preprocessing function to the 'text' column of the dataset\n",
        "df_cleaned['cleaned_text'] = df_cleaned['text'].apply(clean_text)\n",
        "\n",
        "# Prepare data for Word2Vec training (each row is a list of words)\n",
        "sentences = df_cleaned['cleaned_text'].tolist()\n",
        "\n",
        "# Step 3: Train the Word2Vec model using the processed text\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, sg=1, epochs=10)\n",
        "\n",
        "# Step 4: Function to generate sentence embeddings by averaging word embeddings\n",
        "def get_sentence_embedding(sentence, model):\n",
        "    # Get embeddings for each word in the sentence, if present in the model's vocabulary\n",
        "    word_embeddings = [model.wv[word] for word in sentence if word in model.wv]\n",
        "\n",
        "    # Return the average of the word embeddings; handle empty cases\n",
        "    if len(word_embeddings) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    else:\n",
        "        return np.mean(word_embeddings, axis=0)\n",
        "\n",
        "# Generate embeddings for the entire dataset\n",
        "X = np.array([get_sentence_embedding(sentence, word2vec_model) for sentence in df_cleaned['cleaned_text']])\n",
        "y = df_cleaned['class']\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression classifier\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi8kjKiegGRD",
        "outputId": "1a37718e-ec57-4298-e532-e9c383aae1c8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.878\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non-suicide       0.91      0.85      0.88       520\n",
            "     suicide       0.85      0.90      0.88       480\n",
            "\n",
            "    accuracy                           0.88      1000\n",
            "   macro avg       0.88      0.88      0.88      1000\n",
            "weighted avg       0.88      0.88      0.88      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the same examples for classification\n",
        "tricky_text = [\"Lately, I’ve been struggling a lot. There are days when I feel completely overwhelmed, \"\n",
        "               \"like everything is crashing down around me, and I just want to escape. But then there are moments \"\n",
        "               \"where I think maybe things could get better, that I might find a way through this. I’ve been trying \"\n",
        "               \"to reach out to friends, and they’ve been supportive, but it’s hard to explain what I’m going through. \"\n",
        "               \"Some days are okay, but other days, the darkness just feels too heavy to bear. I wish I could see a \"\n",
        "               \"light at the end of the tunnel, but it’s not always there. I just don’t know what to do anymore.\"]\n",
        "\n",
        "normal_text = [\"I’ve been dealing with depression for a few years now, and it hasn’t been easy. There were times \"\n",
        "               \"when I felt like giving up, but I found strength in seeking help. Therapy and talking to friends \"\n",
        "               \"really made a difference. Now, I’m in a much better place, and I want to use my experience to support \"\n",
        "               \"others who might be going through something similar. Mental health is so important, and I believe we \"\n",
        "               \"need to talk about it openly. If sharing my story can encourage even one person to seek help, then \"\n",
        "               \"it’s worth it.\"]\n",
        "\n",
        "# Function to clean new text for the Word2Vec classifier\n",
        "def preprocess_new_text(text):\n",
        "    # Clean and tokenize the text\n",
        "    cleaned_text = clean_text(text)\n",
        "    # Generate sentence embeddings using the Word2Vec model\n",
        "    return get_sentence_embedding(cleaned_text, word2vec_model)\n",
        "\n",
        "# Preprocess and get embeddings for the tricky and normal examples\n",
        "tricky_text_embedding = preprocess_new_text(tricky_text[0])\n",
        "normal_text_embedding = preprocess_new_text(normal_text[0])\n",
        "\n",
        "# Reshape the embeddings for prediction\n",
        "tricky_prediction = classifier.predict(tricky_text_embedding.reshape(1, -1))\n",
        "normal_prediction = classifier.predict(normal_text_embedding.reshape(1, -1))\n",
        "\n",
        "# Print out the results\n",
        "print(\"Tricky Example Prediction:\", tricky_prediction[0])\n",
        "print(\"Normal Example Prediction:\", normal_prediction[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPsRRsN5gRLP",
        "outputId": "985504a1-1f81-4519-f9ad-5629030e6236"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tricky Example Prediction: suicide\n",
            "Normal Example Prediction: suicide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jOL5Odrihano"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fact that the **Word2Vec-based classifier** predicted \"suicide\" for both examples, including the normal supportive message, highlights some of the **limitations** of using Word2Vec embeddings for nuanced text classification. Let’s discuss these limitations and explore potential improvements.\n",
        "\n",
        "### **Limitations of Word2Vec in Text Classification**\n",
        "\n",
        "1. **Lack of Contextual Understanding**:\n",
        "   - **Static Embeddings**: Word2Vec generates static word embeddings, meaning each word has a fixed vector representation regardless of the context in which it appears. For example, \"depression\" in \"overcoming depression\" and \"struggling with depression\" will be represented by the same vector.\n",
        "   - **Misclassification Due to Overlap**: This static nature can lead to misclassification, as seen in the normal example where the model detected keywords like \"depression\" and \"giving up\" but failed to interpret the overall positive tone of the message.\n",
        "\n",
        "2. **Limited Ability to Handle Polysemy (Multiple Meanings)**:\n",
        "   - **No Context Sensitivity**: Since Word2Vec doesn’t differentiate between different meanings of a word, it struggles with polysemous words (e.g., \"support\" as in emotional support vs. physical support). This can lead to incorrect predictions.\n",
        "   - **Semantic Similarity Isn’t Always Enough**: Words that are semantically similar might not always indicate the same sentiment or category. For example, \"support\" might appear in both helpful and distressing contexts, causing confusion for the classifier.\n",
        "\n",
        "3. **Simple Averaging May Lose Information**:\n",
        "   - **Averaging Word Embeddings**: When generating sentence embeddings, we simply average the embeddings of all the words. This process loses information about word order, dependencies, and nuances in meaning, which can be crucial for understanding sentiments and contexts.\n",
        "   - **Ignores Important Words**: If a sentence contains a mix of positive and negative phrases, simple averaging might not accurately capture the dominant sentiment or message.\n",
        "\n",
        "### **How to Improve Further: Potential Solutions**\n",
        "\n",
        "1. **Use Contextual Embeddings (BERT or Other Transformers)**:\n",
        "   - **Context-Aware Representations**: Unlike Word2Vec, models like BERT generate dynamic embeddings that change based on the context. For example, \"depression\" would have different embeddings when used in supportive vs. distressing contexts.\n",
        "   - **Better Handling of Nuance and Polysemy**: BERT can differentiate meanings based on context, making it more suitable for understanding complex and nuanced sentences.\n",
        "   - **Improved Sentence Embeddings**: Pre-trained transformer models like Sentence-BERT (SBERT) can directly generate embeddings for whole sentences, preserving their meaning more effectively than simply averaging word embeddings.\n",
        "\n",
        "   **Implementation Step**:\n",
        "   - Replace Word2Vec with a transformer-based model (e.g., BERT, RoBERTa).\n",
        "   - Use libraries like `transformers` from Hugging Face to generate embeddings.\n",
        "   - Fine-tune the model on your specific dataset for even better performance.\n",
        "\n",
        "2. **Enhanced Preprocessing and Fine-Tuning**:\n",
        "   - **Identify Sentiment Words**: Create a list of key phrases or words that strongly indicate a positive or negative sentiment. Pay special attention to phrases that can change the meaning (e.g., \"not struggling,\" \"getting better\").\n",
        "   - **Domain-Specific Fine-Tuning**: Fine-tune a pre-trained BERT model on a dataset similar to yours (mental health discussions) to make it more accurate in detecting subtle differences in sentiment.\n",
        "   \n",
        "3. **Advanced Embedding Techniques**:\n",
        "   - **Weighted Averaging**: Instead of simple averaging, assign weights to word embeddings based on their importance (e.g., TF-IDF weights). This gives more influence to critical words in the sentence and can help capture the essence of the message more accurately.\n",
        "   - **Attention Mechanisms**: Use attention-based mechanisms to focus on specific parts of the sentence that matter most (e.g., giving more weight to distressing words and less to neutral ones). This can be implemented with transformer architectures.\n",
        "\n",
        "4. **Combine Word2Vec and Other Features**:\n",
        "   - **Incorporate Linguistic Features**: Enhance the classification model by adding linguistic features (e.g., word sentiment scores, presence of negation, frequency of certain keywords). This hybrid approach can help the classifier better understand the text.\n",
        "   - **Use Multiple Embedding Models**: Combine embeddings from Word2Vec, GloVe, and BERT, or use ensemble techniques to leverage the strengths of different models.\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "The limitations of **Word2Vec** for nuanced text classification stem from its static, context-insensitive nature. For more precise and robust classification, especially with texts that carry subtle differences in meaning, **contextual embeddings** like those from BERT offer significant improvements:\n",
        "- **Dynamic Context**: BERT adjusts its understanding based on context, which allows it to accurately capture nuances.\n",
        "- **Better Generalization**: Fine-tuning on domain-specific data allows BERT to generalize better to new examples.\n",
        "\n",
        "**Recommendation**: Transition to **contextual embeddings** by using BERT or similar transformer-based models to address the limitations observed. This shift can lead to more accurate, reliable, and robust classification for complex text inputs.\n"
      ],
      "metadata": {
        "id": "tGlbfY9rhyiK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjezepW6h2OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach-3: LSTM"
      ],
      "metadata": {
        "id": "f_3u5KzKiRYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. What is an LSTM?**\n",
        "\n",
        "**Long Short-Term Memory (LSTM)** is a special kind of **Recurrent Neural Network (RNN)** capable of learning long-term dependencies. Standard RNNs suffer from the **vanishing gradient problem**, where the gradients of the loss function become too small during backpropagation through time, leading to the loss of information over long sequences. LSTMs address this problem by introducing a more sophisticated memory mechanism.\n",
        "\n",
        "#### **Key Concepts in LSTMs:**\n",
        "\n",
        "1. **Recurrent Neural Networks (RNNs)**:\n",
        "   - RNNs are designed to handle sequential data by maintaining a **hidden state** that is updated at each time step. They take the previous state and the current input to produce an output and update the state.\n",
        "   - **Limitation**: RNNs struggle with learning long-term dependencies because the gradient may diminish as it backpropagates through many layers, known as the **vanishing gradient problem**. This makes it difficult for RNNs to learn from data where long-term context is crucial.\n",
        "\n",
        "2. **LSTMs - Overcoming RNN Limitations**:\n",
        "   - LSTMs are an extension of RNNs, specifically designed to overcome the vanishing gradient problem. They can maintain and use information over longer periods.\n",
        "   - The core idea of LSTMs is the **cell state** (memory cell), which acts as a conveyor belt that runs through the entire sequence. The LSTM can add or remove information from this cell state using structures called **gates**.\n",
        "\n",
        "### **Key Features of LSTMs:**\n",
        "\n",
        "1. **Memory Cells**:\n",
        "   - LSTMs maintain a **cell state**, which is a dedicated memory that flows through the network without undergoing too many changes. The network can carry forward important information from previous time steps, allowing it to learn long-term dependencies.\n",
        "   - This memory cell is crucial for retaining information over long sequences, helping the model understand complex patterns in sequential data.\n",
        "\n",
        "2. **Gates Mechanism**:\n",
        "   - **Gates** are the building blocks of LSTMs that control how information flows in and out of the cell state. Each gate has a different function:\n",
        "     - **Forget Gate**: Decides what information to discard from the cell state. It looks at the current input and the previous hidden state and determines which parts of the information are no longer relevant.\n",
        "     - **Input Gate**: Determines what new information should be added to the cell state. It decides how much of the new input should be written into the memory.\n",
        "     - **Output Gate**: Controls the information that should be output from the current cell state to the hidden state. This output is used as input for the next time step.\n",
        "   - These gates make LSTMs more flexible, allowing them to learn when to remember and when to forget, making them adept at processing sequential information.\n",
        "\n",
        "3. **Sequential Understanding**:\n",
        "   - LSTMs read data sequentially, meaning they can preserve the order of words. This allows them to understand context over sequences, making them suitable for tasks like text classification, sentiment analysis, machine translation, and more.\n",
        "   - By learning how each word relates to the next, LSTMs can capture dependencies that may be spread over long distances in the text.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why Use LSTM for This Task?**\n",
        "\n",
        "LSTMs have several properties that make them especially useful for tasks like **text classification**, where the sequence and context of words are critical to understanding meaning.\n",
        "\n",
        "1. **Sequential Context Understanding**:\n",
        "   - LSTMs can process entire sequences of text, preserving the order of words, which allows them to understand how words interact within a sentence. For example, they can distinguish between **\"I am feeling great today\"** and **\"I am not feeling great today\"**, where the word \"not\" changes the sentiment entirely.\n",
        "   - This makes them effective for understanding nuanced differences in context, enabling them to capture complex language patterns.\n",
        "\n",
        "2. **Handling Long Dependencies**:\n",
        "   - In many texts, the meaning or sentiment may depend on words or phrases spread out across the entire sentence or even across multiple sentences. For instance, **\"Although I am struggling now, I’m getting better\"** requires the model to retain information from the beginning of the sentence to understand that it conveys a positive outlook.\n",
        "   - LSTMs can learn to retain and use such information effectively, making them ideal for tasks where long-term context is crucial.\n",
        "\n",
        "3. **Dynamic Contextual Embeddings**:\n",
        "   - While traditional word embeddings like **Word2Vec** generate static vectors, LSTMs can learn dynamic embeddings that change depending on the context within the sequence. For example, the word **\"bank\"** will have different meanings based on whether it is used in **\"river bank\"** or **\"financial bank,\"** and LSTMs can adjust accordingly.\n",
        "   - This allows LSTMs to provide more relevant context, even if they are not as sophisticated as transformer-based models like BERT.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Limitations of LSTMs Compared to Transformers**\n",
        "\n",
        "Despite their advantages, LSTMs have some limitations, especially when compared to more modern approaches like transformer models (e.g., BERT, GPT):\n",
        "\n",
        "1. **Sequential Processing**:\n",
        "   - **LSTMs process data sequentially**, meaning they handle one word at a time. This sequential nature makes training slower because it cannot parallelize the processing of words. In contrast, **transformers** process the entire sentence simultaneously, leading to faster training and inference, especially on large datasets.\n",
        "   - Transformers can leverage parallel computation, making them more scalable for large datasets and longer sequences.\n",
        "\n",
        "2. **Less Powerful Context Understanding**:\n",
        "   - While LSTMs can learn long-term dependencies, their ability to capture very **long-range dependencies** (e.g., across paragraphs) is limited. They might struggle with very long sentences where the important information is spread out.\n",
        "   - **Transformers**, with their self-attention mechanism, excel at understanding relationships between words, no matter how far apart they are in the text. This allows transformers to capture more complex relationships and understand nuanced meanings better than LSTMs.\n",
        "\n",
        "3. **Memory and Computation Issues**:\n",
        "   - LSTMs require significant computational resources to remember and process long sequences, and their performance can degrade when handling very long text data.\n",
        "   - Transformers, with their ability to parallelize and attention mechanisms, handle longer sequences more efficiently and provide better generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Steps to Build an LSTM-Based Text Classifier**\n",
        "\n",
        "Here’s how you can implement an LSTM-based text classification model:\n",
        "\n",
        "1. **Preprocess the Text**:\n",
        "   - **Cleaning**: Remove unwanted characters, punctuation, and convert text to lowercase.\n",
        "   - **Tokenization**: Split the text into individual words or tokens.\n",
        "   - **Convert to Sequences**: Use tokenizers to convert text into sequences of word indices, where each word is represented by its corresponding index.\n",
        "\n",
        "2. **Pad Sequences**:\n",
        "   - Ensure that all input sequences are of the same length by **padding** (adding zeros) or truncating longer sequences. This makes it easier to process batches of data in parallel.\n",
        "\n",
        "3. **Create Word Embeddings**:\n",
        "   - Use pre-trained embeddings like **GloVe** or **Word2Vec**, or train embeddings directly on your dataset. The embeddings transform words into dense vectors that capture semantic meaning.\n",
        "   - The **Embedding Layer** in an LSTM model converts word indices into vectors of fixed size, helping the LSTM learn patterns.\n",
        "\n",
        "4. **Build the LSTM Model**:\n",
        "   - Use an **Embedding Layer** followed by an **LSTM Layer**. The LSTM will learn to capture patterns in the sequences and retain relevant information for classification.\n",
        "   - Add **Dense Layers** and an **Output Layer** with an activation function (e.g., sigmoid for binary classification) to generate predictions.\n",
        "\n",
        "5. **Train and Evaluate**:\n",
        "   - Train the model using the training dataset, and adjust parameters like **learning rate**, **batch size**, and **epochs** to improve performance.\n",
        "   - Evaluate the model’s performance on unseen data (test set) and use metrics like **accuracy, precision, recall**, and **F1-score** to gauge effectiveness.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rCHYqFA5iUa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTMs are not embedding models; they are sequential models** designed to process and learn from sequences of data, such as text, time series, or any other ordered data. Here’s how they differ from embedding models and how they can still use embeddings effectively:\n",
        "\n",
        "### **1. LSTMs vs. Embedding Models:**\n",
        "\n",
        "1. **LSTMs (Sequential Models):**\n",
        "   - **Purpose**: LSTMs are a type of **Recurrent Neural Network (RNN)** designed to capture patterns in sequences. They excel at learning dependencies between elements in a sequence, especially when those dependencies span over long distances.\n",
        "   - **Function**: LSTMs process data step-by-step, maintaining a hidden state that gets updated at each time step. They decide what information to retain or forget as they move through the sequence.\n",
        "   - **Use Case**: Suitable for tasks like **text classification**, **time series forecasting**, **speech recognition**, and **language modeling**. They focus on **sequential processing** rather than generating static or dynamic word vectors.\n",
        "\n",
        "2. **Embedding Models:**\n",
        "   - **Purpose**: Embedding models like **Word2Vec, GloVe, and BERT** are designed to create dense vector representations (embeddings) of words or sentences. These vectors capture semantic information about the words, allowing the model to understand relationships and similarities between them.\n",
        "   - **Function**: Embedding models learn representations by analyzing co-occurrences and patterns in large corpora. For example, Word2Vec learns to place similar words close together in the vector space, while BERT creates contextual embeddings that change based on the word’s role in the sentence.\n",
        "   - **Use Case**: Used to convert words into numerical vectors that can be processed by other models. Embeddings can be used in any downstream task, such as **search engines**, **recommendation systems**, **text classification**, and **language translation**.\n",
        "\n",
        "### **2. How LSTMs Use Embeddings:**\n",
        "\n",
        "While LSTMs themselves are not embedding models, they often **use embeddings as input** to improve their performance:\n",
        "\n",
        "1. **Embedding Layer as Input**:\n",
        "   - In a typical LSTM-based text classification model, an **embedding layer** is used at the start to convert words into dense vectors (embeddings).\n",
        "   - These embeddings can be **pre-trained** (like GloVe, Word2Vec, or BERT embeddings) or can be **learned** during the training process. The LSTM then processes these embeddings sequentially to capture patterns across the sequence.\n",
        "   - For example, if you have a sentence like \"I love programming,\" the embedding layer will convert each word into a vector. The LSTM will read these vectors one by one, understand their relationships, and generate an output.\n",
        "\n",
        "2. **Dynamic Learning**:\n",
        "   - When an LSTM model is used for a specific task, it can learn to update the embeddings during training, tailoring them to be more effective for that particular task.\n",
        "   - This means that even if the model starts with generic embeddings, they can become more specialized over time based on the data and task requirements.\n",
        "\n",
        "### **3. Why This Distinction Matters:**\n",
        "\n",
        "1. **LSTMs Are About Learning Sequential Patterns**:\n",
        "   - LSTMs are powerful because they can retain information over long sequences and understand how different parts of a sequence interact. This makes them useful for tasks where the **order** of elements matters, like **language modeling** or **time series forecasting**.\n",
        "   - Their core strength is their ability to maintain and update a hidden state over time, rather than just representing the input data as static vectors.\n",
        "\n",
        "2. **Embedding Models Are About Creating Meaningful Representations**:\n",
        "   - Embedding models focus on generating **compact, informative representations** of data. These representations can be used as inputs for other models (like LSTMs, transformers, or traditional classifiers).\n",
        "   - For example, **Word2Vec** learns static embeddings that can be used to find similar words, while **BERT** creates contextual embeddings that understand the nuances of language.\n",
        "\n",
        "### **4. Practical Example: Combining Embeddings and LSTMs**\n",
        "\n",
        "If you want to build a **text classification model** using LSTMs, you can follow this approach:\n",
        "1. **Use an Embedding Layer**: Convert words to vectors (using pre-trained embeddings like GloVe or by learning your own embeddings during training).\n",
        "2. **Feed Embeddings into an LSTM**: The LSTM will process these embeddings sequentially, retaining the important information across the sequence and generating a final output.\n",
        "3. **Classification**: Use the final output from the LSTM to make predictions (e.g., classify the sentiment of a sentence).\n",
        "\n",
        "### **Conclusion:**\n",
        "\n",
        "- **LSTMs are sequential models** designed to process sequences by retaining and learning dependencies over time.\n",
        "- **Embedding models generate vector representations** of data that capture relationships and similarities.\n",
        "- **Combination**: LSTMs can leverage embeddings to better understand the input data, but they are not responsible for generating the embeddings themselves.\n",
        "\n",
        "LSTMs and embeddings work **together** to create powerful models that can understand and classify sequential data, but they serve different purposes. While embeddings convert text to numerical vectors, LSTMs analyze those vectors over time to find patterns and dependencies.\n"
      ],
      "metadata": {
        "id": "eLJV4edzjkJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = 'data_mental_health.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop unnecessary columns if any (e.g., index column)\n",
        "df_cleaned = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
        "\n",
        "# Convert target labels to numeric\n",
        "df_cleaned['class'] = df_cleaned['class'].apply(lambda x: 1 if x == 'suicide' else 0)\n",
        "\n",
        "# Clean and preprocess the text data\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation and special characters\n",
        "    return text\n",
        "\n",
        "df_cleaned['cleaned_text'] = df_cleaned['text'].apply(clean_text)\n",
        "\n",
        "# Step 2: Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=10000)  # Use top 10,000 words in the vocabulary\n",
        "tokenizer.fit_on_texts(df_cleaned['cleaned_text'])\n",
        "X = tokenizer.texts_to_sequences(df_cleaned['cleaned_text'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "X = pad_sequences(X, maxlen=100)  # Adjust maxlen as needed\n",
        "y = df_cleaned['class']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Build the LSTM Model\n",
        "model = Sequential()\n",
        "# Embedding layer (learned during training)\n",
        "model.add(Embedding(input_dim=10000, output_dim=128, input_length=100))  # Embedding size is 128\n",
        "model.add(SpatialDropout1D(0.2))  # Add dropout for regularization\n",
        "# LSTM layer\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "# Dense layer for classification\n",
        "model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Train the model\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1, callbacks=[early_stop])\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Predict new examples\n",
        "tricky_text = [\"Lately, I’ve been struggling a lot. There are days when I feel completely overwhelmed, \"\n",
        "               \"like everything is crashing down around me, and I just want to escape. But then there are moments \"\n",
        "               \"where I think maybe things could get better, that I might find a way through this. I’ve been trying \"\n",
        "               \"to reach out to friends, and they’ve been supportive, but it’s hard to explain what I’m going through. \"\n",
        "               \"Some days are okay, but other days, the darkness just feels too heavy to bear. I wish I could see a \"\n",
        "               \"light at the end of the tunnel, but it’s not always there. I just don’t know what to do anymore.\"]\n",
        "\n",
        "normal_text = [\"I’ve been dealing with depression for a few years now, and it hasn’t been easy. There were times \"\n",
        "               \"when I felt like giving up, but I found strength in seeking help. Therapy and talking to friends \"\n",
        "               \"really made a difference. Now, I’m in a much better place, and I want to use my experience to support \"\n",
        "               \"others who might be going through something similar. Mental health is so important, and I believe we \"\n",
        "               \"need to talk about it openly. If sharing my story can encourage even one person to seek help, then \"\n",
        "               \"it’s worth it.\"]\n",
        "\n",
        "# Preprocess and predict\n",
        "tricky_seq = pad_sequences(tokenizer.texts_to_sequences(tricky_text), maxlen=100)\n",
        "normal_seq = pad_sequences(tokenizer.texts_to_sequences(normal_text), maxlen=100)\n",
        "\n",
        "print(\"Tricky Example Prediction:\", model.predict(tricky_seq))\n",
        "print(\"Normal Example Prediction:\", model.predict(normal_seq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybVHQ2eLkNOo",
        "outputId": "ee1d8c52-6d44-4561-9084-6ef58a263dc3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 247ms/step - accuracy: 0.6666 - loss: 0.6156 - val_accuracy: 0.8425 - val_loss: 0.3743\n",
            "Epoch 2/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 222ms/step - accuracy: 0.8661 - loss: 0.3599 - val_accuracy: 0.8675 - val_loss: 0.3014\n",
            "Epoch 3/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 217ms/step - accuracy: 0.9254 - loss: 0.2172 - val_accuracy: 0.8700 - val_loss: 0.3091\n",
            "Epoch 4/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 220ms/step - accuracy: 0.9404 - loss: 0.1679 - val_accuracy: 0.8875 - val_loss: 0.2821\n",
            "Epoch 5/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 219ms/step - accuracy: 0.9500 - loss: 0.1400 - val_accuracy: 0.8825 - val_loss: 0.3214\n",
            "Epoch 6/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 208ms/step - accuracy: 0.9688 - loss: 0.0967 - val_accuracy: 0.8575 - val_loss: 0.3708\n",
            "Epoch 7/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 227ms/step - accuracy: 0.9735 - loss: 0.0887 - val_accuracy: 0.8850 - val_loss: 0.3702\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8751 - loss: 0.3415\n",
            "Test Accuracy: 0.87\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
            "Tricky Example Prediction: [[0.9863618]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Normal Example Prediction: [[0.96521986]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjW6nNSqknQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}